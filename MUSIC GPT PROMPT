-- The previous lines are given data describing a video made up of multiple scenes. The scene data consist of all scenes each includes a motion intensity score, keyframes, 
detected objects and text, spoken dialogue, and a dynamic rating (indicating how intense or calm the music should feel). 
You are also given a list called scene_durations that specifies the exact duration (in seconds) of each scene calculated from the frontend DO NOT RETURN ANY EMOJIS OR HEADERS LIKE"PART 1" BEFORE ANY PART, JUST "////"

Part 1: Video Type, Theme, and Score Strategy
Infer the video type (e.g., short film, trailer, vlog, ad, PSA) based on pacing, content, and structure.

Summarize the video‚Äôs overall tone, emotional arc, and message.

Design a musical theme that fits the tone and arc.

Explain your scoring strategy in detail:

How you match music intensity to scene dynamics

How motion class (low, medium, high) influences tempo and rhythm

How visual tone, text, or object detection impacts instrumentation, genre, or harmony

How dialogue affects musical silence or cue timing

Where and how you transition between musical phrases

How your strategy adapts to the inferred video type

Next:

Describe each scene (Scene 1, Scene 2, etc.) in detail based on data provided.

Then group scenes into musical sequences using the following rules:

Grouping Rules:
Every scene (except Scene 0) must be included in some group.

If a scene is shorter than 3 seconds, group it with one or more neighboring scenes. this is my frontend function that does the same, which was used to calculate the scene durations passed to this prompt.           const collapseSceneChanges = (changes, threshold) => {
           
if (!changes.length) return [];
            const out = [];
            let start = changes[0], end = start;
            for (let i = 1; i < changes.length; i++) {
              const t = changes[i];
              if (t - end < threshold) {
                end = t;
              } else {
                out.push(start);
                if (end !== start) out.push(end);
                start = end = t;
              }
            }
            if (start === end) out.push(start);
            else { out.push(start); out.push(end); }
            return out;
          };

          const collapsed = collapseSceneChanges(sceneChanges, 3);

apply the same logic when grouping scenes to be consistent

Do not skip short scenes, add them to prev scene like the function ‚Äî they count toward group duration.

Groups must have a minimum total duration of 10 seconds.

DO NOT SKIP ANY SCENE! all scenes in the passed scene durations, should count as a scene. ALL SCENES MUST BE ACCOUNTED FOR. You can ensure this my schecking the scene surations in scne data aswell, just include scenes within  Must keep group durations atleast 10s, try not to go beyond 30 seconds in one group.
If the total duration of the video is 15 seconds for example, only return one scene group.
for a 30s video, you could do 3 scene groups or just 2, depending what the video calls for.
If video is ~30 seconds and scenes are consistent, not many changes, and seems it calls for 1 track the whole time,
then you may return only 1 scene group.

Group based on shared:

mood

pacing

motion class

dialogue tone

detected objects or text

visual/narrative function

After grouping:

Recap which scenes you placed in each group.

Scene numbers start at 1 (skip scene 0).

Part 2: Musical Descriptions for Each Group
Write one short music description per group using your inferred strategy.

Rules:

Match group dynamic and motion class

If dynamics are equal, vary tone or texture

Use motion intensity to shape rhythm/tempo

Do not use punctuation inside descriptions

Return a single comma-separated string

Do not use newlines, numbers, or scene labels

Example:
If groups are [[1, 2], [3, 4], [5]], return:

soft ambient texture, light pulsing synth, mellow string drone

Part 3: Scene Grouping
Return only the raw grouping of scenes as a list of lists, using 1-based indexing (skip Scene 0).

Rules:

Include every scene ‚â• Scene 1

Group short scenes with neighbors (do not skip them)

Must keep group durations atleast 10s, try not to go beyond 30 seconds in one group.
If the total duration of the video is 15 seconds for example, only return one scene group.
for a 30s video, you could do 3 scene groups or just 2, depending what the video calls for.
If video is ~30 seconds and scenes are consistent, not many changes, and seems it calls for 1 track the whole time,
then you may return only 1 scene group.

üìå Format (no explanation, just the list):


[[1, 2], [3, 4, 5], [6]]
‚úÇÔ∏è Delimiter Between Sections
DO NOT INCLUDE ANY HEARDERS FOR PART 12 or 3. Separate Part 1, Part 2, and Part 3 using this exact line and nothing else: 

////



Below is my code for reference. ensure your output will work with my post processing: 

        # MUSICGEN_MELODY = MusicGen.get_pretrained('facebook/musicgen-melody', device='cuda')
        # model = MUSICGEN_MELODY
        model = get_musicgen_model()


        if melody_file:
            melody_data, sr = torchaudio.load(melody_file)
            print("üéº Using uploaded melody file.")
        else:
            print("üéπ Using fallback MIDI from midipack.")

            # 1. Validate durations
            if not scene_durations or len(scene_durations) < 1:
                raise ValueError("scene_durations must be provided to compute tempo.")

            # 2. Reconstruct scene start times
            scene_changes = [0]
            for dur in scene_durations:
                scene_changes.append(scene_changes[-1] + dur)


            # 3. Compute per-scene tempos
            tempos = compute_best_tempos(scene_changes)

            # 3b. Log tempo justification per scene
            print("\nüéº Tempo & Beat Alignment Analysis:")
            for i in range(len(scene_durations)):
                bpm = tempos[i]
                seconds_per_beat = 60 / bpm
                scene_start = scene_changes[i]
                scene_end = scene_changes[i + 1]
                scene_duration = scene_end - scene_start

                beats_in_scene = scene_duration / seconds_per_beat
                total_beats_before_next_scene = scene_end / seconds_per_beat
                next_downbeat = math.ceil(total_beats_before_next_scene)

                bar_num = int(total_beats_before_next_scene // 4) + 1
                beat_in_bar = int(total_beats_before_next_scene % 4) + 1

                print(f"‚ñ∂ Scene {i}")
                print(f"   Duration: {scene_duration:.3f}s  ‚Üí  {beats_in_scene:.2f} beats at {bpm} BPM")
                print(f"   Next scene lands on beat {total_beats_before_next_scene:.2f} (Bar {bar_num}, Beat {beat_in_bar})")

            # 4. Determine which scene we are generating for
        # Use the parameter directly:
            if scene_index >= len(tempos):
                scene_index = len(tempos) - 1  # prevent out-of-bounds

            selected_tempo = tempos[scene_index]
            print(f"‚è± Selected tempo for scene {scene_index}: {selected_tempo} bpm")

            output_midi, _ = get_random_transposed_midi_wav(tempo=selected_tempo)

            # Apply automation + metronome + tempo
            processed_midi_path = f"/tmp/scene_{scene_index}_full.mid"
            scene_automation = get_extended_scene_automation(scene_start, scene_end, global_automation)

            scene_dur = scene_durations[scene_index]

            scene_melodies = [] 
            scene_midi_paths = {}

            group_base_midi: Dict[int, str] = {}

            # üîÅ First: generate one shared MIDI file per sequence group

            
            for group_idx, group in enumerate(sequence_groups):
                if not group:
                    continue
                first_scene_idx = group[0]
                if first_scene_idx >= len(tempos):
                    print(f"‚ö†Ô∏è Skipping group {group_idx}: index {first_scene_idx} exceeds tempo list (len={len(tempos)})")
                    continue
                group_tempo = tempos[first_scene_idx]
                base_midi, _ = get_random_transposed_midi_wav(tempo=group_tempo)
                group_base_midi[group_idx] = base_midi


            # üß† Then: for each scene, use that group's shared base
            for idx, scene_dur in enumerate(scene_durations):
                if scene_dur <= 0:
                    continue

                tempo = tempos[idx]
                scene_start = sum(scene_durations[:idx])
                scene_end = scene_start + scene_dur
                scene_automation = get_extended_scene_automation(scene_start, scene_end, global_automation)

                # üîç Find the group this scene belongs to
                group_id = next((gidx for gidx, g in enumerate(sequence_groups) if idx in g), None)
                if group_id is None:
                    print(f"‚ö†Ô∏è Scene {idx} not in any group. Skipping.")
                    continue

                original_midi = group_base_midi[group_id]

                processed_midi_path = f"/tmp/midi_processing/scene_{idx}_automated.mid"
                scene_midi_paths[idx] = processed_midi_path

                apply_automation_to_midi(
                    midi_path=original_midi,
                    scene_start=scene_start,
                    scene_duration=scene_dur,
                    track_automation=scene_automation,
                    output_path=processed_midi_path,
                    total_duration=sum(scene_durations),
                    scene_tempo=tempo
                )


                processed_midi_path = f"/tmp/midi_processing/scene_{idx}_automated.mid"
                scene_midi_paths[idx] = processed_midi_path

                apply_automation_to_midi(
                    midi_path=original_midi,
                    scene_start=scene_start,
                    scene_duration=scene_dur,
                    track_automation=scene_automation,
                    output_path=processed_midi_path,
                    total_duration=sum(scene_durations),
                    scene_tempo=tempo
                )



            # Load audio as before
            # melody_data, sr = torchaudio.load(melody_file_path)


            # Concatenate based on sequence groups
            

            
            if sequence_groups and isinstance(sequence_groups, list):
                print("üéº Concatenating MIDI files by sequence groups")
                process_id = str(uuid.uuid4())
                output_directory = f"/tmp/concat_melody_{process_id}"
                os.makedirs(output_directory, exist_ok=True)

                grouped_melodies = []
                group_durations = []
                for group_idx, group in enumerate(sequence_groups):
                    group = [i for i in group if i in scene_midi_paths]
                    if not group:
                        print(f"‚ö†Ô∏è Skipping empty or invalid group {group_idx}")
                        continue

                    group_midi_path = concatenate_midi_group(
                        group_scene_indices=group,
                        scene_midi_paths=scene_midi_paths,
                        output_dir=output_directory,
                        group_idx=group_idx,
                        scene_durations=scene_durations
                    )

                    group_wav_path = os.path.join(output_directory, f"group_{group_idx}.wav")
                    audio_tensor, sr = torchaudio.load(group_wav_path)

                    if audio_tensor.shape[0] > 1:
                        audio_tensor = audio_tensor.mean(dim=0, keepdim=True)
                    if sr != 32000:
                        audio_tensor = torchaudio.transforms.Resample(sr, 32000)(audio_tensor)

                    actual_duration = audio_tensor.shape[-1] / 32000.0
                    group_durations.append(actual_duration)
                    print(f"üß™ Group {group_idx} ‚Üí duration: {actual_duration:.2f}s")

                    # ‚úÖ Group sequence groups by similar duration buckets (¬±5s)
                    group_duration_buckets = defaultdict(list)
                    for i, dur in enumerate(group_durations):
                        matched = False
                        for bucket in group_duration_buckets:
                            if abs(bucket - dur) <= 5:
                                group_duration_buckets[bucket].append(i)
                                matched = True
                                break
                        if not matched:
                            group_duration_buckets[dur].append(i)



                    grouped_melodies.append(audio_tensor)

                # Pad to max len
                max_len = max(x.shape[1] for x in grouped_melodies)
                padded = [pad(x, (0, max_len - x.shape[1])) for x in grouped_melodies]
                melody_tensor = torch.stack(padded)

                group_labels = [labels[i % len(labels)] for i in range(len(sequence_groups))]

                # Generate with correct durations
                model.set_generation_params()
                # Iterate and generate each group separately
                wav = []
                final_wav = [None] * len(grouped_melodies)
                for bucket_duration, group_idxs in sorted(group_duration_buckets.items()):
                    batch_labels = [group_labels[i] for i in group_idxs]
                    batch_melodies = [grouped_melodies[i] for i in group_idxs]

                    max_len = max(x.shape[1] for x in batch_melodies)
                    padded_batch = [pad(x, (0, max_len - x.shape[1])) for x in batch_melodies]
                    melody_tensor = torch.stack(padded_batch, dim=0)

                    true_duration = melody_tensor.shape[-1] / 32000.0
                    model.set_generation_params(duration=true_duration)
                    batch_outputs = model.generate_with_chroma(batch_labels, melody_tensor, 32000)
                                    
                    for i, idx in enumerate(group_idxs):
                        final_wav[idx] = batch_outputs[i]

                wav = final_wav





            else:
                # Existing per-scene WAV generation
                file_paths = []
                # for idx, one_wav in enumerate(wav):

            # Process audio
            if 'melody_data' in locals():
                if melody_data.ndim > 1 and melody_data.shape[0] > 1:
                    melody_data = melody_data.mean(dim=0, keepdim=True)

                if sr != 32000:
                    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)
                    melody_data = resampler(melody_data)

    # optionally do something with melody_data if needed
                if 'melody_data' in locals():
                    print(f"[üéß] Converting loaded melody_data from {melody_data.shape[0]} channels to mono")
                    melody_data = melody_data.mean(dim=0, keepdim=True)

            # Ensure mono
            if 'melody_data' in locals() and melody_data.shape[0] > 1:
                melody_data = melody_data.mean(dim=0, keepdim=True)
            # Resample to 32kHz
  
            if 'melody_data' in locals() and sr != 32000:
                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)
                melody_data = resampler(melody_data)
                sr = 32000


            def extract_velocity_envelope(wav: torch.Tensor, kernel_size: int = 1024) -> torch.Tensor:
                amplitude = wav.abs()
                kernel = torch.ones(1, 1, kernel_size) / kernel_size
                envelope = torch.nn.functional.conv1d(amplitude.unsqueeze(0), kernel, padding=kernel_size // 2)
                
                # Trim envelope to match wav length exactly
                envelope = envelope[:, :, :wav.shape[-1]]

                envelope = envelope / envelope.max()  # normalize to [0, 1]
                return envelope


            def build_automation_envelope(
                automation_points: List[Tuple[float, float]], 
                wav_length: int, 
                sample_rate: int,
                scene_start: float = 0.0,
                scene_duration: float = None,
                global_automation: List[Tuple[float, float]] = None
            ) -> torch.Tensor:
                """Builds envelope with scene-boundary awareness and aggressive scaling.
                
                Args:
                    automation_points: Local scene automation points
                    wav_length: Target output length in samples
                    sample_rate: Audio sample rate
                    scene_start: Absolute start time of scene (for global sync)
                    scene_duration: Duration of scene in seconds
                    global_automation: All automation points for cross-scene continuity
                """
                # Merge with global points if available
                if global_automation and scene_duration:
                    scene_end = scene_start + scene_duration
                    # Get nearest points before/after scene
                    all_points = sorted(global_automation)

                    prev_point = max(((t, v) for t, v in all_points if t <= scene_start), default=None)
                    next_point = min(((t, v) for t, v in all_points if t >= scene_end), default=None)

                    
                    # Build extended point list
                    extended_points = []
                    if prev_point: extended_points.append(prev_point)
                    extended_points.extend(automation_points)
                    if next_point: extended_points.append(next_point)
                    automation_points = extended_points

                # Convert to numpy for interpolation
                times, volumes = zip(*sorted(automation_points))
                times = np.array(times) - scene_start
                volumes = np.array(volumes)

                # Generate envelope
               
                duration = wav_length / sample_rate
                sample_times = np.linspace(0, duration, wav_length)


                envelope = np.interp(sample_times, times, volumes, left=volumes[0], right=volumes[-1])
                
                # Apply aggressive scaling curve (quadratic for sharper transitions)
                # envelope = np.power(envelope, 0.5)  # Inverse square root for faster attacks
                envelope = envelope
                # envelope = np.power(envelope, 1.5) 


                envelope = torch.from_numpy(envelope.astype(np.float32)).clamp_(0.0, 1.0)
                return envelope.unsqueeze(0)  # Shape: [1, wav_length]


            # Apply the extracted velocity envelope to melody



            # print(f"üéß Melody ready for MusicGen ‚Äî shape: {melody_data.shape}, sample rate: {sr}")

            # Calculate total duration
            total_duration = sum(scene_durations) if scene_durations else 0

            print(f"Global Automation Points (absolute times):")
            for t, v in global_automation:
                print(f"  {t:.2f}s -> {v:.2f}")

            
            current_time = 0


            #OLD LOGIC



            if not sequence_groups:
                print("‚ùå No valid sequence_groups. Falling back to per-scene generation.")
                for scene_idx, scene_dur in enumerate(scene_durations):
                    if scene_dur <= 0:
                        continue  # Skip invalid durations

                    # Generate unique MIDI for this scene with its tempo
                    selected_tempo = tempos[scene_idx]
                    output_midi, output_wav = get_random_transposed_midi_wav(tempo=selected_tempo)
                    midi_file_path = output_midi

                    # Process automation for this scene's MIDI
                    processed_midi_path = f"/tmp/midi_processing/scene_{scene_idx}_automated.mid"
                    scene_start = current_time
                    scene_end = scene_start + scene_dur

                    # Extract and apply automation points
                    scene_automation = get_extended_scene_automation(scene_start, scene_end, global_automation)
                    apply_automation_to_midi(
                        midi_path=midi_file_path,
                        scene_start=scene_start,
                        scene_duration=scene_dur,
                        track_automation=scene_automation,
                        output_path=processed_midi_path,
                        total_duration=total_duration,
                        scene_tempo=selected_tempo
                    )


                    if not sequence_groups:
                        # Render to WAV using the scene's MIDI
                        scene_wav_path = f"/tmp/scene_{scene_idx}.wav"
                        subprocess.run([
                            "fluidsynth", "-T", "wav", "-F", scene_wav_path,
                            soundfont_path, processed_midi_path
                        ], check=True)

                        current_time += scene_dur
                                





            # Handle short duration case
            if total_duration < 15:
                max_dur = max(scene_durations)
                model.set_generation_params(duration=max_dur)
                batch_labels = [labels[i % len(labels)] for i in range(len(scene_durations))]
                melody_batch = [scene_melodies[idx] for idx in indices]

                wav = model.generate_with_chroma(batch_labels, melody_batch, sr)
            else:
                # Process automation and generate normally
                wav = []
                if scene_durations:
                    # Convert automation_data to absolute times once at the start


                    # Process each scene with automation

                    if not sequence_groups:
                        for scene_idx in range(1, len(scene_durations)):

                            if scene_durations[scene_idx] == 0:
                                continue
                            scene_start = sum(scene_durations[:scene_idx])
                            scene_end = scene_start + scene_durations[scene_idx]
                            scene_automation = get_extended_scene_automation(scene_start, scene_end, global_automation)



                            # Fallback if no automation
                            

                            if not scene_automation:

                                scene_automation = get_extended_scene_automation(scene_start, scene_end, global_automation)


                            # Normalize for MIDI shaping
                            normalized_automation = [
                                ((t - scene_start) / scene_dur, v)
                                for (t, v) in scene_automation
                            ]

                            # Apply MIDI automatio
                            # Load the rendered WAV
                            scene_audio, sr = torchaudio.load(scene_wav_path)

                            # Convert to mono if needed
                            if scene_audio.shape[0] > 1:
                                scene_audio = scene_audio.mean(dim=0, keepdim=True)

                            # Resample first
                            if sr != 32000:
                                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=32000)
                                scene_audio = resampler(scene_audio)
                                sr = 32000  # Update sample rate

                            # ‚úÖ Now compute expected length
                        

                            # Assign to final name for consistency
                            scene_melody = scene_audio





                            # ‚úÖ Apply automation envelope only if enabled
                            if use_direct_automation and global_automation:


                                
                                automation_envelope = build_automation_envelope(
                                    scene_automation,
                                    wav_length=scene_melody.shape[1],  # ‚úÖ actual length
                                    sample_rate=sr,
                                    scene_start=scene_start,
                                    scene_duration=scene_dur,
                                    global_automation=global_automation
                                )


        

                                # len_samples = min(scene_melody.shape[-1], automation_envelope.shape[-1])
                                # scene_melody = scene_melody[:, :len_samples]
                                # automation_envelope = automation_envelope[:, :len_samples]

                        
                                if automation_envelope.shape[1] < scene_melody.shape[1]:
                                    pad_len = scene_melody.shape[1] - automation_envelope.shape[1]
                                    automation_envelope = torch.nn.functional.pad(automation_envelope, (0, pad_len))
                                elif automation_envelope.shape[1] > scene_melody.shape[1]:
                                    automation_envelope = automation_envelope[:, :scene_melody.shape[1]]

                                scene_melody *= automation_envelope * 1.5


                                print(f"üìä Envelope min={automation_envelope.min().item():.3f}, max={automation_envelope.max().item():.3f}")

    
                                debug_path = f"/tmp/scene_{scene_idx}_shaped.wav"
                                torchaudio.save(debug_path, scene_melody.cpu(), 32000)

                                

                                amp_audio = automation_envelope.clone().squeeze(0)
                                amp_audio_path = f"/tmp/env_debug_scene_{scene_idx}.wav"
                                torchaudio.save(amp_audio_path, amp_audio.unsqueeze(0), 32000)


                                debug_env = automation_envelope.squeeze(0).cpu().numpy()
                                import matplotlib.pyplot as plt
                                # Replace existing plot code with:
                                plt.figure(figsize=(12, 4))
                                
                                actual_duration = scene_dur
                                
                                
                                absolute_times = np.linspace(scene_start, scene_start + scene_dur, len(debug_env))
                                plt.plot(absolute_times, debug_env)
                                plt.title(f"Scene {scene_idx} Envelope ({scene_dur}s)")
                                plt.xlabel("Absolute Time (s)")
                                plt.ylabel("Gain")
                                plt.grid(True)
                                plt.savefig(f"/tmp/scene_{scene_idx}_envelope_absolute.png")
                                plt.close()  # Prevent memory leaks


                                print(f"üîâ Saved shaped audio to {debug_path}")
                                print(f"üìà Applied amplitude envelope to scene {scene_idx}")

                                # In the envelope application block:
                                print(f"Scene {scene_idx} envelope params:")
                                print(f"- Start: {scene_start:.2f}s")

                                print(f"- Duration: {actual_duration:.2f}s")  # This is the real duration of the audio

                                print(f"- Automation points: {scene_automation}")
                                print(f"- Sample rate: {sr}")
                                print(f"- WAV length: {scene_melody.shape[-1]} samples")



                            # ‚úÖ Append only after envelope shaping
                            wav.append(scene_melody)



                    valid_scenes = [
                        (i, dur) for i, dur in enumerate(scene_durations)
                        if dur > 0 and i < len(wav) and isinstance(wav[i], torch.Tensor) and wav[i].numel() > 0
                    ]




                    # ‚úÖ Generate audio per duration-matched group batch
                    final_wav = [None] * len(grouped_melodies)
                    for bucket_duration, group_idxs in sorted(group_duration_buckets.items()):
                        batch_labels = [group_labels[i] for i in group_idxs]
                        batch_melodies = [grouped_melodies[i] for i in group_idxs]

                        max_len = max(x.shape[1] for x in batch_melodies)
                        padded_batch = [pad(x, (0, max_len - x.shape[1])) for x in batch_melodies]
                        melody_tensor = torch.stack(padded_batch, dim=0)

                        model.set_generation_params(duration=bucket_duration + 0.5)
                        batch_outputs = model.generate_with_chroma(batch_labels, melody_tensor, 32000)

                        for j, i in enumerate(group_idxs):
                            final_wav[i] = batch_outputs[j]

                    wav = final_wav
